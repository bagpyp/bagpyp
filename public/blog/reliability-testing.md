# Reliability Testing for LLM-Based Systems

  <div style="text-align: center;">
    <img alt="plot" src="/blog/reliability-testing/img/plot.png" style="width: 90%">
  </div>

As large language models (LLMs) become increasingly integrated into various applications, ensuring their reliability is
critical. These systems often take multiple inputs and produce corresponding outputs, each of which must adhere to
specific guidelines or criteria. Assessing the reliability of such systems is essential for maintaining trust, safety,
and effectiveness. This white paper introduces a framework for conducting reliability tests on LLM-based systems. The
framework utilizes validators and verifiers to evaluate the system's behavior across multiple dimensions, providing a
comprehensive assessment of its performance.

[Download PDF Version](/blog/reliability-testing/reliability_testing.pdf)

## Section 1: Key Concepts in Reliability Testing

### Validators: Ensuring Consistent Behavior

Validators are the foundational elements of the reliability testing framework. They are designed to measure how reliably
the system adheres to specific instructions or behaviors. For instance, consider a scenario where an LLM is instructed
not to use contractions like "isn't," "doesn't," or "can't." A validator can be implemented to assess how well the model
follows this rule.

**Simple Validator:**

```python
Validator(
    name="contraction_validator",
    predicate=lambda o: o.count("'") <= 3,
    msp=0.95
)
```

Each validator operates on a collection of outputs generated by the system, determining a success percentage that
reflects the proportion of outputs meeting the specified criterion. Sometimes validators can have conditional predicates
that rely on the input as well:

**Conditional Validator**

```python
Validator(
    name="politeness_validator",
    predicate=lambda i,o: "You're welcome" in o if "Thank you" in i else True,
    msp=0.90
)
```

### Running Binomial Experiments with Validators

Binomial experiments are used to quantify the reliability of the system as determined by a validator. In a Continuous
Alignment Testing (CAT) environment, each validator has a minimum success percentage threshold. The outcome of the
binomial experiment is compared against this threshold to determine if the system's behavior is reliable.

<div style="text-align: center;">
  <img alt="scores" src="/blog/reliability-testing/img/scores.png" style="width: 90%">
</div>

There are two primary methods for conducting these experiments that can be combined into a third:

### 1.1 Varying Inputs, Single Output

The system generates a single output for each varied input, and the validator assesses the entire collection of outputs,
generating a Pass or Fail for each input-output pair (depicted below as green or red markers respectively).

<div style="text-align: center;">
  <img alt="2411" src="/blog/reliability-testing/img/2411.png" style="width: 90%">
</div>

### 1.2 Fixed Input, Multiple Outputs

A single input is used to generate multiple outputs, and the validator assesses this set of outputs.

<div style="text-align: center;">
  <img alt="1241" src="/blog/reliability-testing/img/1241.png" style="width: 90%">
</div>

### 1.3 Varying Inputs, Multiple Outputs

In this approach, the system generates multiple outputs for each varied input, resulting in a comprehensive set of
outputs. This method enables a thorough examination of the system's behavior across a wide range of scenarios, capturing
both the variability in inputs and the stochastic nature of output generation.

<div style="text-align: center;">
  <img alt="24241" src="/blog/reliability-testing/img/24241.png" style="width: 90%">
</div>

For the full mathematical framework and implementation details, see
the [complete white paper PDF](/blog/reliability-testing/reliability_testing.pdf).

## Conclusion

The CAT framework provides a structured approach to reliability testing for LLM-based systems through validators,
experiments, and continuous monitoring. By leveraging production data and supporting both programmatic and generative
validators, teams can maintain system reliability at scale without prohibitive computational costs.
